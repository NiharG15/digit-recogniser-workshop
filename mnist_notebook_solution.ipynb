{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recogniser using Convolutional Neural Network\n",
    "\n",
    "\n",
    "In this workshop we are going a develop and train a deep neural network called convolutional neural network to recognise handwritten digits. We'll be using the MNIST (\"Modified National Institute of Standards and Technology\") dataset, also considered as the \"Hello World\" dataset in computer vision/deep learning. It contains a large set of human handwritten and annotated digit images.\n",
    "\n",
    "By the end of the tutorial you will have learnt how convolutional neural networks work, how to deploy pre-trained model, how to visualise the results. We'll build this neural net using tensorflow and deploy the pretrained model as rest api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Verify TensorFlow is Installed\n",
    "\n",
    "We'll start of by first importing the libraries that are required for this project. The Deep learning framework we are using is tensorflow. This is even to check if the correct version of tensorflow is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data Set\n",
    "\n",
    "MNIST Data Set is a collection of handwritten digits images comprising 60,000 training images and 10,000 test images. It usually used as a benchmark for the new computer vision and pattern recognition algorithms. The images here have been size-normalised and centered in fixed size image.\n",
    "\n",
    "Tensorflow has a module which helps us download this particular dataset, split it into training set, validation set, test set and creates one-hot vectors for labels of each image. The training set contains 55,000 images, validation set has 5,000 and test set has 10,000 images.\n",
    "\n",
    "**So why is it important to separate data into 3 sets?**\n",
    "\n",
    "These images here are 2D images each consisting an array of 28x28 values that have been flattened to get a rich structure of 784 dimensional vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Training Examples :  55000\n",
      "No. of Validation Examples :  5000\n",
      "No. of Test Examples :  10000\n",
      "Example of a one-hot encoded vector : \n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "Flattened Image Shape :  (784,)\n",
      "Example of a flattened image: \n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.38039219  0.37647063\n",
      "  0.3019608   0.46274513  0.2392157   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.35294119  0.5411765\n",
      "  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869\n",
      "  0.98431379  0.98431379  0.97254908  0.99607849  0.96078438  0.92156869\n",
      "  0.74509805  0.08235294  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.54901963  0.98431379  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.74117649  0.09019608\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.88627458  0.99607849  0.81568635\n",
      "  0.78039223  0.78039223  0.78039223  0.78039223  0.54509807  0.2392157\n",
      "  0.2392157   0.2392157   0.2392157   0.2392157   0.50196081  0.8705883\n",
      "  0.99607849  0.99607849  0.74117649  0.08235294  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14901961  0.32156864  0.0509804   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.13333334  0.83529419  0.99607849  0.99607849  0.45098042  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.41568631  0.6156863   0.99607849  0.99607849  0.95294124  0.20000002\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.09803922  0.45882356  0.89411771\n",
      "  0.89411771  0.89411771  0.99215692  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.94117653  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.26666668  0.4666667   0.86274517\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.55686277  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.14509805  0.73333335  0.99215692\n",
      "  0.99607849  0.99607849  0.99607849  0.87450987  0.80784321  0.80784321\n",
      "  0.29411766  0.26666668  0.84313732  0.99607849  0.99607849  0.45882356\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.44313729\n",
      "  0.8588236   0.99607849  0.94901967  0.89019614  0.45098042  0.34901962\n",
      "  0.12156864  0.          0.          0.          0.          0.7843138\n",
      "  0.99607849  0.9450981   0.16078432  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.66274512  0.99607849  0.6901961   0.24313727  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.18823531\n",
      "  0.90588242  0.99607849  0.91764712  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.07058824  0.48627454  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.32941177  0.99607849  0.99607849  0.65098041  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.54509807  0.99607849  0.9333334   0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.82352948  0.98039222  0.99607849  0.65882355  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.94901967  0.99607849  0.93725497  0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.34901962  0.98431379  0.9450981   0.33725491  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01960784  0.80784321  0.96470594  0.6156863   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.01568628  0.45882356  0.27058825  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Training Examples : \",mnist.train.num_examples)\n",
    "print(\"No. of Validation Examples : \",mnist.validation.num_examples)\n",
    "print(\"No. of Test Examples : \",mnist.test.num_examples)\n",
    "print(\"Example of a one-hot encoded vector : \\n\",mnist.train.labels[0])\n",
    "print(\"Flattened Image Shape : \", mnist.train.images[0].shape)\n",
    "print(\"Example of a flattened image: \\n\", mnist.train.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqJ9SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcbfa740518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_array = mnist.train.images[np.random.randint(mnist.train.images.shape[0])]\n",
    "img_array = 255 * img_array\n",
    "img_array = img_array.astype(\"uint8\")\n",
    "#print(img_array.reshape([28,28]))\n",
    "plt.imshow(img_array.reshape([28,28]))\n",
    "plt.gray()\n",
    "plt.grid(True)\n",
    "plt.savefig('test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our network\n",
    "\n",
    "You'll have to build the necessary components given below alongwith us for designing the convolutional neural network.\n",
    "- `input_placeholders`\n",
    "- `init_weights_bias`\n",
    "- `conv2d`\n",
    "- `max_pool`\n",
    "- `flatten`\n",
    "- `fcn`\n",
    "- `output`\n",
    "- `cnn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "Write Something about the 4 placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_placeholders():\n",
    "    \"\"\"\n",
    "    Create TF placeholders for inputs, targets, keep_prob, learning_rate\n",
    "    Return: A Tuple(inputs, targets, keep_prob, learning_rate)\n",
    "    \"\"\"\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, 784], name=\"Inputs\")\n",
    "    targets = tf.placeholder(tf.float32, shape=[None, 10], name=\"Labels\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    \n",
    "    return inputs, targets, keep_prob, learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights_bias(w_shape, b_shape):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal(w_shape, stddev=0.1), name=\"weights\")\n",
    "    bias = tf.Variable(tf.truncated_normal(b_shape), name=\"bias\")\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(inputs, weights, stride):\n",
    "    \"\"\"\n",
    "    Apply convolution operation to inputs tensor.\n",
    "    inputs: tensorflow tensor\n",
    "    weights: weights initialised for convolution operation\n",
    "    stride: 2d tuple for convolution\n",
    "    \"\"\"\n",
    "    return tf.nn.conv2d(inputs, weights, strides=[1,stride[0], stride[1], 1],name=\"conv\" ,padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(inputs, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Apply max pooling operation to inputs tensor.\n",
    "    inputs: tensorflow tensor\n",
    "    kernel_size: 2d tuple for pooling\n",
    "    stride: 2d tuple for pooling\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(inputs, ksize=[1, kernel_size[0],kernel_size[1],1], \n",
    "                          strides=[1, stride[0], stride[1], 1], padding=\"SAME\", name=\"max_pool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(inputs):\n",
    "    \"\"\"\n",
    "    Flatten inputs tensor to (batch_size, flattened_image_size).\n",
    "    \"\"\"\n",
    "    shape_ip = inputs.get_shape().as_list()\n",
    "    return tf.reshape(inputs,[-1, shape_ip[1] * shape_ip[2] * shape_ip[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fcn(inputs, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to inputs tensor using weights and bias\n",
    "    inputs: tensorflow tensor\n",
    "    num_outputs: number of outputs the new tensor should be.\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([inputs.get_shape().as_list()[1], num_outputs], stddev=0.1), name=\"weights\")\n",
    "    bias = tf.Variable(tf.truncated_normal([num_outputs]), name=\"bias\")\n",
    "    \n",
    "    fc = tf.add(tf.matmul(inputs, weights), bias)\n",
    "    fc = tf.nn.relu(fc)\n",
    "    \n",
    "    tf.summary.histogram(\"weights\", weights)\n",
    "    tf.summary.histogram(\"biases\", bias)\n",
    "    tf.summary.histogram(\"activations\", fc)\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputs(inputs, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply an output layer to inputs tensor using weights and bias\n",
    "    inputs: tensorflow tensor\n",
    "    num_outputs: number of outputs the new tensor should be.\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([inputs.get_shape().as_list()[1], num_outputs], stddev=0.1), name=\"weights\")\n",
    "    bias = tf.Variable(tf.truncated_normal([num_outputs]), name=\"weights\")\n",
    "    outputs = tf.add(tf.matmul(inputs, weights), bias, name=\"outputs\")\n",
    "    \n",
    "    tf.summary.histogram(\"weights\", weights)\n",
    "    tf.summary.histogram(\"biases\", bias)\n",
    "    tf.summary.histogram(\"activations\", outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn(inputs, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional model using inputs tensor and use keep_prob while applying dropout \n",
    "    \"\"\"\n",
    "    inputs = tf.reshape(inputs, [-1, 28, 28, 1])\n",
    "    \n",
    "    tf.summary.image('input', inputs, 3)\n",
    "    \n",
    "    #Conv layer 1\n",
    "    with tf.name_scope(\"conv_1\"):\n",
    "        wc1, bc1 = init_weights_bias([5, 5, 1, 32], [32])\n",
    "        c1 = conv2d(inputs, wc1, [1,1])+bc1\n",
    "        r1 = tf.nn.relu(c1)\n",
    "        m1 = max_pool(r1, [2,2], [2,2])\n",
    "    \n",
    "    #Conv layer 2\n",
    "    with tf.name_scope(\"conv_2\"):\n",
    "        wc2, bc2 = init_weights_bias([5,5,32,64],[64])\n",
    "        c2 = conv2d(m1, wc2, [1,1])+bc2\n",
    "        r2 = tf.nn.relu(c2)\n",
    "        m2 = max_pool(r2, [2,2], [2,2])\n",
    "    \n",
    "    #Flatten the output of last conv layer\n",
    "    flat_m2 = flatten(m2)\n",
    "    \n",
    "    #Fully connected layer\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fcn1 = fcn(flat_m2, 1024)\n",
    "    \n",
    "    #Dropout\n",
    "    drp1 = tf.nn.dropout(fcn1, keep_prob)\n",
    "    \n",
    "    #Output layer\n",
    "    with tf.name_scope(\"output_fc_layer\"):\n",
    "        out = outputs(drp1, 10)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_stats(sess, batch_train, batch_train_labels, batch_val, batch_val_labels, cost, accuracy, writer, step):\n",
    "    \"\"\"\n",
    "    Display intermediate results.\n",
    "    \"\"\"\n",
    "    train_data = sess.run([accuracy, summ, cost], feed_dict={\n",
    "        inputs: batch_train, targets: batch_train_labels, keep_prob: 1.0, learning_rate:0.0001})\n",
    "    val_data =sess.run([accuracy, cost], feed_dict={\n",
    "        inputs: batch_val, targets: batch_val_labels, keep_prob: 1.0, learning_rate:0.0001})\n",
    "    \n",
    "    writer.add_summary(train_data[1])\n",
    "    \n",
    "    print(\"Step {} Train_Loss : {:>10.4f} Train_Accuracy : {:.6f} Val_Loss : {:>10.4f} Val_Accuracy : {:.6f}\"\n",
    "         .format(step, train_data[2], train_data[0], val_data[1], val_data[0]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#Initialise Session\n",
    "sess = tf.Session()\n",
    "\n",
    "#Inputs\n",
    "inputs, targets, keep_prob, learning_rate = input_placeholders()\n",
    "\n",
    "#Model\n",
    "logits = cnn(inputs, keep_prob)\n",
    "\n",
    "#Loss Function\n",
    "with tf.name_scope(\"Loss_Function\"):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=logits))\n",
    "    tf.summary.scalar(\"loss\", cross_entropy)\n",
    "\n",
    "#Optimizer\n",
    "with tf.name_scope(\"Optimizer\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# Report Accuracy\n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(targets, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "#Define variable to save summaries\n",
    "summ = tf.summary.merge_all()\n",
    "\n",
    "#Initialise all variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Create a saver object to save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#Write Graph to Tensorboard\n",
    "!rm -r log/\n",
    "writer = tf.summary.FileWriter(\"log/\")\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Train_Loss :    32.3006 Train_Accuracy : 0.120000 Val_Loss :    30.4499 Val_Accuracy : 0.097600\n",
      "Step 100 Train_Loss :     0.8387 Train_Accuracy : 0.720000 Val_Loss :     0.8054 Val_Accuracy : 0.742200\n",
      "Step 200 Train_Loss :     0.6210 Train_Accuracy : 0.780000 Val_Loss :     0.5171 Val_Accuracy : 0.843800\n",
      "Step 300 Train_Loss :     0.4699 Train_Accuracy : 0.840000 Val_Loss :     0.4126 Val_Accuracy : 0.889400\n",
      "Step 400 Train_Loss :     0.4179 Train_Accuracy : 0.880000 Val_Loss :     0.3483 Val_Accuracy : 0.905600\n",
      "Step 500 Train_Loss :     0.2449 Train_Accuracy : 0.960000 Val_Loss :     0.3157 Val_Accuracy : 0.914800\n",
      "Step 600 Train_Loss :     0.1916 Train_Accuracy : 0.980000 Val_Loss :     0.2830 Val_Accuracy : 0.920600\n",
      "Step 700 Train_Loss :     0.2709 Train_Accuracy : 0.920000 Val_Loss :     0.2608 Val_Accuracy : 0.929400\n",
      "Step 800 Train_Loss :     0.1990 Train_Accuracy : 0.960000 Val_Loss :     0.2323 Val_Accuracy : 0.936600\n",
      "Step 900 Train_Loss :     0.1322 Train_Accuracy : 0.960000 Val_Loss :     0.2198 Val_Accuracy : 0.933400\n",
      "Step 1000 Train_Loss :     0.2262 Train_Accuracy : 0.920000 Val_Loss :     0.2032 Val_Accuracy : 0.944200\n",
      "Step 1100 Train_Loss :     0.1160 Train_Accuracy : 0.960000 Val_Loss :     0.1876 Val_Accuracy : 0.946800\n",
      "Step 1200 Train_Loss :     0.3477 Train_Accuracy : 0.900000 Val_Loss :     0.1838 Val_Accuracy : 0.945400\n",
      "Step 1300 Train_Loss :     0.1693 Train_Accuracy : 0.960000 Val_Loss :     0.1663 Val_Accuracy : 0.951800\n",
      "Step 1400 Train_Loss :     0.0855 Train_Accuracy : 0.980000 Val_Loss :     0.1707 Val_Accuracy : 0.951800\n",
      "Step 1500 Train_Loss :     0.1293 Train_Accuracy : 0.940000 Val_Loss :     0.1562 Val_Accuracy : 0.956600\n",
      "Step 1600 Train_Loss :     0.1836 Train_Accuracy : 0.940000 Val_Loss :     0.1525 Val_Accuracy : 0.959600\n",
      "Step 1700 Train_Loss :     0.2830 Train_Accuracy : 0.900000 Val_Loss :     0.1461 Val_Accuracy : 0.958200\n",
      "Step 1800 Train_Loss :     0.0957 Train_Accuracy : 0.980000 Val_Loss :     0.1435 Val_Accuracy : 0.959000\n",
      "Step 1900 Train_Loss :     0.1471 Train_Accuracy : 0.960000 Val_Loss :     0.1312 Val_Accuracy : 0.962000\n",
      "Step 2000 Train_Loss :     0.2258 Train_Accuracy : 0.900000 Val_Loss :     0.1400 Val_Accuracy : 0.958000\n",
      "Step 2100 Train_Loss :     0.1316 Train_Accuracy : 0.940000 Val_Loss :     0.1290 Val_Accuracy : 0.961800\n",
      "Step 2200 Train_Loss :     0.1996 Train_Accuracy : 0.920000 Val_Loss :     0.1125 Val_Accuracy : 0.966800\n",
      "Step 2300 Train_Loss :     0.1174 Train_Accuracy : 0.940000 Val_Loss :     0.1117 Val_Accuracy : 0.968600\n",
      "Step 2400 Train_Loss :     0.0486 Train_Accuracy : 0.980000 Val_Loss :     0.1098 Val_Accuracy : 0.968200\n",
      "Step 2500 Train_Loss :     0.0486 Train_Accuracy : 1.000000 Val_Loss :     0.1027 Val_Accuracy : 0.970000\n",
      "Step 2600 Train_Loss :     0.1945 Train_Accuracy : 0.940000 Val_Loss :     0.1012 Val_Accuracy : 0.969200\n",
      "Step 2700 Train_Loss :     0.0430 Train_Accuracy : 1.000000 Val_Loss :     0.0968 Val_Accuracy : 0.973600\n",
      "Step 2800 Train_Loss :     0.0558 Train_Accuracy : 0.980000 Val_Loss :     0.1019 Val_Accuracy : 0.970200\n",
      "Step 2900 Train_Loss :     0.0268 Train_Accuracy : 1.000000 Val_Loss :     0.0932 Val_Accuracy : 0.972600\n",
      "Step 3000 Train_Loss :     0.1048 Train_Accuracy : 0.960000 Val_Loss :     0.0900 Val_Accuracy : 0.973800\n",
      "Step 3100 Train_Loss :     0.1524 Train_Accuracy : 0.980000 Val_Loss :     0.0859 Val_Accuracy : 0.973800\n",
      "Step 3200 Train_Loss :     0.0287 Train_Accuracy : 1.000000 Val_Loss :     0.0845 Val_Accuracy : 0.974600\n",
      "Step 3300 Train_Loss :     0.0781 Train_Accuracy : 0.980000 Val_Loss :     0.0942 Val_Accuracy : 0.972400\n",
      "Step 3400 Train_Loss :     0.0577 Train_Accuracy : 0.980000 Val_Loss :     0.0857 Val_Accuracy : 0.974000\n",
      "Step 3500 Train_Loss :     0.0850 Train_Accuracy : 0.960000 Val_Loss :     0.0818 Val_Accuracy : 0.974600\n",
      "Step 3600 Train_Loss :     0.2102 Train_Accuracy : 0.920000 Val_Loss :     0.0837 Val_Accuracy : 0.974400\n",
      "Step 3700 Train_Loss :     0.0675 Train_Accuracy : 0.980000 Val_Loss :     0.0821 Val_Accuracy : 0.974200\n",
      "Step 3800 Train_Loss :     0.0804 Train_Accuracy : 0.960000 Val_Loss :     0.0754 Val_Accuracy : 0.978600\n",
      "Step 3900 Train_Loss :     0.0125 Train_Accuracy : 1.000000 Val_Loss :     0.0813 Val_Accuracy : 0.974400\n",
      "Step 4000 Train_Loss :     0.0376 Train_Accuracy : 1.000000 Val_Loss :     0.0774 Val_Accuracy : 0.976600\n",
      "Step 4100 Train_Loss :     0.0170 Train_Accuracy : 1.000000 Val_Loss :     0.0736 Val_Accuracy : 0.976800\n",
      "Step 4200 Train_Loss :     0.0179 Train_Accuracy : 1.000000 Val_Loss :     0.0694 Val_Accuracy : 0.977800\n",
      "Step 4300 Train_Loss :     0.0374 Train_Accuracy : 0.980000 Val_Loss :     0.0714 Val_Accuracy : 0.978600\n",
      "Step 4400 Train_Loss :     0.1158 Train_Accuracy : 0.980000 Val_Loss :     0.0761 Val_Accuracy : 0.977200\n",
      "Step 4500 Train_Loss :     0.0774 Train_Accuracy : 0.980000 Val_Loss :     0.0722 Val_Accuracy : 0.978200\n",
      "Step 4600 Train_Loss :     0.0790 Train_Accuracy : 0.980000 Val_Loss :     0.0692 Val_Accuracy : 0.978200\n",
      "Step 4700 Train_Loss :     0.0580 Train_Accuracy : 0.980000 Val_Loss :     0.0719 Val_Accuracy : 0.976800\n",
      "Step 4800 Train_Loss :     0.0192 Train_Accuracy : 1.000000 Val_Loss :     0.0675 Val_Accuracy : 0.979600\n",
      "Step 4900 Train_Loss :     0.1075 Train_Accuracy : 0.960000 Val_Loss :     0.0651 Val_Accuracy : 0.978600\n",
      "Step 5000 Train_Loss :     0.0329 Train_Accuracy : 1.000000 Val_Loss :     0.0715 Val_Accuracy : 0.979000\n",
      "Step 5100 Train_Loss :     0.0059 Train_Accuracy : 1.000000 Val_Loss :     0.0633 Val_Accuracy : 0.981400\n",
      "Step 5200 Train_Loss :     0.1293 Train_Accuracy : 0.960000 Val_Loss :     0.0667 Val_Accuracy : 0.978000\n",
      "Step 5300 Train_Loss :     0.1123 Train_Accuracy : 0.940000 Val_Loss :     0.0615 Val_Accuracy : 0.982600\n",
      "Step 5400 Train_Loss :     0.0196 Train_Accuracy : 1.000000 Val_Loss :     0.0559 Val_Accuracy : 0.983000\n",
      "Step 5500 Train_Loss :     0.0987 Train_Accuracy : 0.960000 Val_Loss :     0.0561 Val_Accuracy : 0.983600\n",
      "Step 5600 Train_Loss :     0.0785 Train_Accuracy : 0.980000 Val_Loss :     0.0603 Val_Accuracy : 0.982800\n",
      "Step 5700 Train_Loss :     0.0717 Train_Accuracy : 0.940000 Val_Loss :     0.0553 Val_Accuracy : 0.982800\n",
      "Step 5800 Train_Loss :     0.0747 Train_Accuracy : 0.980000 Val_Loss :     0.0611 Val_Accuracy : 0.982400\n",
      "Step 5900 Train_Loss :     0.0216 Train_Accuracy : 1.000000 Val_Loss :     0.0585 Val_Accuracy : 0.982200\n",
      "Step 6000 Train_Loss :     0.0753 Train_Accuracy : 0.980000 Val_Loss :     0.0546 Val_Accuracy : 0.983800\n",
      "Step 6100 Train_Loss :     0.1181 Train_Accuracy : 0.960000 Val_Loss :     0.0563 Val_Accuracy : 0.982800\n",
      "Step 6200 Train_Loss :     0.0412 Train_Accuracy : 1.000000 Val_Loss :     0.0553 Val_Accuracy : 0.982000\n",
      "Step 6300 Train_Loss :     0.1176 Train_Accuracy : 0.960000 Val_Loss :     0.0612 Val_Accuracy : 0.981000\n",
      "Step 6400 Train_Loss :     0.0262 Train_Accuracy : 1.000000 Val_Loss :     0.0517 Val_Accuracy : 0.983600\n",
      "Step 6500 Train_Loss :     0.0730 Train_Accuracy : 0.980000 Val_Loss :     0.0517 Val_Accuracy : 0.983800\n",
      "Step 6600 Train_Loss :     0.2636 Train_Accuracy : 0.960000 Val_Loss :     0.0545 Val_Accuracy : 0.981200\n",
      "Step 6700 Train_Loss :     0.1045 Train_Accuracy : 0.940000 Val_Loss :     0.0559 Val_Accuracy : 0.982600\n",
      "Step 6800 Train_Loss :     0.0269 Train_Accuracy : 0.980000 Val_Loss :     0.0494 Val_Accuracy : 0.984400\n",
      "Step 6900 Train_Loss :     0.0147 Train_Accuracy : 1.000000 Val_Loss :     0.0601 Val_Accuracy : 0.980800\n",
      "Step 7000 Train_Loss :     0.0223 Train_Accuracy : 1.000000 Val_Loss :     0.0498 Val_Accuracy : 0.985800\n",
      "Step 7100 Train_Loss :     0.1228 Train_Accuracy : 0.960000 Val_Loss :     0.0490 Val_Accuracy : 0.985000\n",
      "Step 7200 Train_Loss :     0.1062 Train_Accuracy : 0.960000 Val_Loss :     0.0511 Val_Accuracy : 0.983000\n",
      "Step 7300 Train_Loss :     0.0489 Train_Accuracy : 0.980000 Val_Loss :     0.0492 Val_Accuracy : 0.984600\n",
      "Step 7400 Train_Loss :     0.0061 Train_Accuracy : 1.000000 Val_Loss :     0.0500 Val_Accuracy : 0.985400\n",
      "Step 7500 Train_Loss :     0.0547 Train_Accuracy : 0.960000 Val_Loss :     0.0502 Val_Accuracy : 0.985600\n",
      "Step 7600 Train_Loss :     0.0068 Train_Accuracy : 1.000000 Val_Loss :     0.0515 Val_Accuracy : 0.984800\n",
      "Step 7700 Train_Loss :     0.1534 Train_Accuracy : 0.960000 Val_Loss :     0.0469 Val_Accuracy : 0.984800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7800 Train_Loss :     0.0603 Train_Accuracy : 0.980000 Val_Loss :     0.0441 Val_Accuracy : 0.986800\n",
      "Step 7900 Train_Loss :     0.0075 Train_Accuracy : 1.000000 Val_Loss :     0.0478 Val_Accuracy : 0.986000\n",
      "Step 8000 Train_Loss :     0.0419 Train_Accuracy : 0.980000 Val_Loss :     0.0478 Val_Accuracy : 0.985800\n",
      "Step 8100 Train_Loss :     0.0407 Train_Accuracy : 0.980000 Val_Loss :     0.0463 Val_Accuracy : 0.985200\n",
      "Step 8200 Train_Loss :     0.0109 Train_Accuracy : 1.000000 Val_Loss :     0.0489 Val_Accuracy : 0.986400\n",
      "Step 8300 Train_Loss :     0.0147 Train_Accuracy : 1.000000 Val_Loss :     0.0444 Val_Accuracy : 0.986600\n",
      "Step 8400 Train_Loss :     0.0345 Train_Accuracy : 0.980000 Val_Loss :     0.0457 Val_Accuracy : 0.986000\n",
      "Step 8500 Train_Loss :     0.0358 Train_Accuracy : 0.980000 Val_Loss :     0.0477 Val_Accuracy : 0.984800\n",
      "Step 8600 Train_Loss :     0.0963 Train_Accuracy : 0.940000 Val_Loss :     0.0462 Val_Accuracy : 0.985800\n",
      "Step 8700 Train_Loss :     0.1218 Train_Accuracy : 0.980000 Val_Loss :     0.0440 Val_Accuracy : 0.987000\n",
      "Step 8800 Train_Loss :     0.0031 Train_Accuracy : 1.000000 Val_Loss :     0.0454 Val_Accuracy : 0.986200\n",
      "Step 8900 Train_Loss :     0.0040 Train_Accuracy : 1.000000 Val_Loss :     0.0420 Val_Accuracy : 0.987800\n",
      "Step 9000 Train_Loss :     0.0028 Train_Accuracy : 1.000000 Val_Loss :     0.0434 Val_Accuracy : 0.987600\n",
      "Step 9100 Train_Loss :     0.0558 Train_Accuracy : 0.980000 Val_Loss :     0.0468 Val_Accuracy : 0.985800\n",
      "Step 9200 Train_Loss :     0.0088 Train_Accuracy : 1.000000 Val_Loss :     0.0437 Val_Accuracy : 0.986000\n",
      "Step 9300 Train_Loss :     0.0184 Train_Accuracy : 1.000000 Val_Loss :     0.0449 Val_Accuracy : 0.987600\n",
      "Step 9400 Train_Loss :     0.0398 Train_Accuracy : 0.980000 Val_Loss :     0.0428 Val_Accuracy : 0.989200\n",
      "Step 9500 Train_Loss :     0.0038 Train_Accuracy : 1.000000 Val_Loss :     0.0404 Val_Accuracy : 0.988800\n",
      "Step 9600 Train_Loss :     0.0103 Train_Accuracy : 1.000000 Val_Loss :     0.0431 Val_Accuracy : 0.986400\n",
      "Step 9700 Train_Loss :     0.0304 Train_Accuracy : 0.980000 Val_Loss :     0.0419 Val_Accuracy : 0.988200\n",
      "Step 9800 Train_Loss :     0.1430 Train_Accuracy : 0.960000 Val_Loss :     0.0432 Val_Accuracy : 0.987000\n",
      "Step 9900 Train_Loss :     0.0069 Train_Accuracy : 1.000000 Val_Loss :     0.0394 Val_Accuracy : 0.989200\n",
      "Step 10000 Train_Loss :     0.0848 Train_Accuracy : 0.980000 Val_Loss :     0.0456 Val_Accuracy : 0.986400\n",
      "Step 10100 Train_Loss :     0.0329 Train_Accuracy : 0.980000 Val_Loss :     0.0414 Val_Accuracy : 0.986800\n",
      "Step 10200 Train_Loss :     0.0319 Train_Accuracy : 0.980000 Val_Loss :     0.0423 Val_Accuracy : 0.987200\n",
      "Step 10300 Train_Loss :     0.0023 Train_Accuracy : 1.000000 Val_Loss :     0.0420 Val_Accuracy : 0.988600\n",
      "Step 10400 Train_Loss :     0.0345 Train_Accuracy : 0.980000 Val_Loss :     0.0393 Val_Accuracy : 0.988000\n",
      "Step 10500 Train_Loss :     0.0441 Train_Accuracy : 0.960000 Val_Loss :     0.0453 Val_Accuracy : 0.987800\n",
      "Step 10600 Train_Loss :     0.0189 Train_Accuracy : 1.000000 Val_Loss :     0.0405 Val_Accuracy : 0.987600\n",
      "Step 10700 Train_Loss :     0.1512 Train_Accuracy : 0.980000 Val_Loss :     0.0442 Val_Accuracy : 0.987000\n",
      "Step 10800 Train_Loss :     0.0193 Train_Accuracy : 1.000000 Val_Loss :     0.0409 Val_Accuracy : 0.987600\n",
      "Step 10900 Train_Loss :     0.0053 Train_Accuracy : 1.000000 Val_Loss :     0.0423 Val_Accuracy : 0.987000\n",
      "Step 11000 Train_Loss :     0.0272 Train_Accuracy : 1.000000 Val_Loss :     0.0372 Val_Accuracy : 0.989600\n",
      "Step 11100 Train_Loss :     0.0091 Train_Accuracy : 1.000000 Val_Loss :     0.0387 Val_Accuracy : 0.989000\n",
      "Step 11200 Train_Loss :     0.0476 Train_Accuracy : 0.980000 Val_Loss :     0.0387 Val_Accuracy : 0.989400\n",
      "Step 11300 Train_Loss :     0.0290 Train_Accuracy : 0.980000 Val_Loss :     0.0351 Val_Accuracy : 0.989600\n",
      "Step 11400 Train_Loss :     0.0044 Train_Accuracy : 1.000000 Val_Loss :     0.0404 Val_Accuracy : 0.987600\n",
      "Step 11500 Train_Loss :     0.1009 Train_Accuracy : 0.960000 Val_Loss :     0.0378 Val_Accuracy : 0.988800\n",
      "Step 11600 Train_Loss :     0.0106 Train_Accuracy : 1.000000 Val_Loss :     0.0364 Val_Accuracy : 0.990400\n",
      "Step 11700 Train_Loss :     0.0217 Train_Accuracy : 1.000000 Val_Loss :     0.0358 Val_Accuracy : 0.989000\n",
      "Step 11800 Train_Loss :     0.0333 Train_Accuracy : 0.980000 Val_Loss :     0.0379 Val_Accuracy : 0.989200\n",
      "Step 11900 Train_Loss :     0.0469 Train_Accuracy : 0.980000 Val_Loss :     0.0404 Val_Accuracy : 0.988000\n",
      "Step 12000 Train_Loss :     0.0012 Train_Accuracy : 1.000000 Val_Loss :     0.0422 Val_Accuracy : 0.988800\n",
      "Step 12100 Train_Loss :     0.0028 Train_Accuracy : 1.000000 Val_Loss :     0.0354 Val_Accuracy : 0.990200\n",
      "Step 12200 Train_Loss :     0.0077 Train_Accuracy : 1.000000 Val_Loss :     0.0385 Val_Accuracy : 0.988800\n",
      "Step 12300 Train_Loss :     0.0169 Train_Accuracy : 1.000000 Val_Loss :     0.0369 Val_Accuracy : 0.988600\n",
      "Step 12400 Train_Loss :     0.0455 Train_Accuracy : 0.980000 Val_Loss :     0.0378 Val_Accuracy : 0.989000\n",
      "Step 12500 Train_Loss :     0.0045 Train_Accuracy : 1.000000 Val_Loss :     0.0358 Val_Accuracy : 0.989600\n",
      "Step 12600 Train_Loss :     0.0043 Train_Accuracy : 1.000000 Val_Loss :     0.0356 Val_Accuracy : 0.991000\n",
      "Step 12700 Train_Loss :     0.0702 Train_Accuracy : 0.960000 Val_Loss :     0.0400 Val_Accuracy : 0.988600\n",
      "Step 12800 Train_Loss :     0.0112 Train_Accuracy : 1.000000 Val_Loss :     0.0353 Val_Accuracy : 0.989000\n",
      "Step 12900 Train_Loss :     0.0099 Train_Accuracy : 1.000000 Val_Loss :     0.0374 Val_Accuracy : 0.988600\n",
      "Step 13000 Train_Loss :     0.0232 Train_Accuracy : 1.000000 Val_Loss :     0.0392 Val_Accuracy : 0.989200\n",
      "Step 13100 Train_Loss :     0.0052 Train_Accuracy : 1.000000 Val_Loss :     0.0366 Val_Accuracy : 0.989600\n",
      "Step 13200 Train_Loss :     0.0010 Train_Accuracy : 1.000000 Val_Loss :     0.0406 Val_Accuracy : 0.988600\n",
      "Step 13300 Train_Loss :     0.0693 Train_Accuracy : 0.980000 Val_Loss :     0.0365 Val_Accuracy : 0.989600\n",
      "Step 13400 Train_Loss :     0.0177 Train_Accuracy : 1.000000 Val_Loss :     0.0363 Val_Accuracy : 0.989000\n",
      "Step 13500 Train_Loss :     0.0006 Train_Accuracy : 1.000000 Val_Loss :     0.0371 Val_Accuracy : 0.990200\n",
      "Step 13600 Train_Loss :     0.0109 Train_Accuracy : 1.000000 Val_Loss :     0.0360 Val_Accuracy : 0.989800\n",
      "Step 13700 Train_Loss :     0.0484 Train_Accuracy : 0.960000 Val_Loss :     0.0373 Val_Accuracy : 0.988400\n",
      "Step 13800 Train_Loss :     0.0227 Train_Accuracy : 0.980000 Val_Loss :     0.0386 Val_Accuracy : 0.988800\n",
      "Step 13900 Train_Loss :     0.0286 Train_Accuracy : 1.000000 Val_Loss :     0.0369 Val_Accuracy : 0.988600\n",
      "Step 14000 Train_Loss :     0.0029 Train_Accuracy : 1.000000 Val_Loss :     0.0388 Val_Accuracy : 0.990000\n",
      "Step 14100 Train_Loss :     0.0166 Train_Accuracy : 1.000000 Val_Loss :     0.0362 Val_Accuracy : 0.988600\n",
      "Step 14200 Train_Loss :     0.0025 Train_Accuracy : 1.000000 Val_Loss :     0.0388 Val_Accuracy : 0.988000\n",
      "Step 14300 Train_Loss :     0.0009 Train_Accuracy : 1.000000 Val_Loss :     0.0389 Val_Accuracy : 0.988800\n",
      "Step 14400 Train_Loss :     0.0024 Train_Accuracy : 1.000000 Val_Loss :     0.0383 Val_Accuracy : 0.989800\n",
      "Step 14500 Train_Loss :     0.0075 Train_Accuracy : 1.000000 Val_Loss :     0.0389 Val_Accuracy : 0.989800\n",
      "Step 14600 Train_Loss :     0.0300 Train_Accuracy : 0.980000 Val_Loss :     0.0346 Val_Accuracy : 0.989400\n",
      "Step 14700 Train_Loss :     0.0028 Train_Accuracy : 1.000000 Val_Loss :     0.0363 Val_Accuracy : 0.989800\n",
      "Step 14800 Train_Loss :     0.0267 Train_Accuracy : 0.980000 Val_Loss :     0.0370 Val_Accuracy : 0.989200\n",
      "Step 14900 Train_Loss :     0.0015 Train_Accuracy : 1.000000 Val_Loss :     0.0364 Val_Accuracy : 0.990400\n",
      "Step 15000 Train_Loss :     0.0031 Train_Accuracy : 1.000000 Val_Loss :     0.0365 Val_Accuracy : 0.990000\n",
      "Step 15100 Train_Loss :     0.0041 Train_Accuracy : 1.000000 Val_Loss :     0.0358 Val_Accuracy : 0.990400\n",
      "Step 15200 Train_Loss :     0.0052 Train_Accuracy : 1.000000 Val_Loss :     0.0375 Val_Accuracy : 0.989600\n",
      "Step 15300 Train_Loss :     0.0442 Train_Accuracy : 0.980000 Val_Loss :     0.0392 Val_Accuracy : 0.989200\n",
      "Step 15400 Train_Loss :     0.0048 Train_Accuracy : 1.000000 Val_Loss :     0.0426 Val_Accuracy : 0.988000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15500 Train_Loss :     0.0013 Train_Accuracy : 1.000000 Val_Loss :     0.0354 Val_Accuracy : 0.989400\n",
      "Step 15600 Train_Loss :     0.0009 Train_Accuracy : 1.000000 Val_Loss :     0.0368 Val_Accuracy : 0.989200\n",
      "Step 15700 Train_Loss :     0.0097 Train_Accuracy : 1.000000 Val_Loss :     0.0323 Val_Accuracy : 0.990200\n",
      "Step 15800 Train_Loss :     0.0003 Train_Accuracy : 1.000000 Val_Loss :     0.0330 Val_Accuracy : 0.990200\n",
      "Step 15900 Train_Loss :     0.0013 Train_Accuracy : 1.000000 Val_Loss :     0.0334 Val_Accuracy : 0.990400\n",
      "Step 16000 Train_Loss :     0.0096 Train_Accuracy : 1.000000 Val_Loss :     0.0331 Val_Accuracy : 0.991000\n",
      "Step 16100 Train_Loss :     0.0026 Train_Accuracy : 1.000000 Val_Loss :     0.0338 Val_Accuracy : 0.989800\n",
      "Step 16200 Train_Loss :     0.0008 Train_Accuracy : 1.000000 Val_Loss :     0.0381 Val_Accuracy : 0.988400\n",
      "Step 16300 Train_Loss :     0.0020 Train_Accuracy : 1.000000 Val_Loss :     0.0392 Val_Accuracy : 0.988000\n",
      "Step 16400 Train_Loss :     0.0417 Train_Accuracy : 0.980000 Val_Loss :     0.0359 Val_Accuracy : 0.988600\n",
      "Step 16500 Train_Loss :     0.0078 Train_Accuracy : 1.000000 Val_Loss :     0.0339 Val_Accuracy : 0.990600\n",
      "Step 16600 Train_Loss :     0.0014 Train_Accuracy : 1.000000 Val_Loss :     0.0329 Val_Accuracy : 0.990800\n",
      "Step 16700 Train_Loss :     0.0159 Train_Accuracy : 0.980000 Val_Loss :     0.0381 Val_Accuracy : 0.989600\n",
      "Step 16800 Train_Loss :     0.0002 Train_Accuracy : 1.000000 Val_Loss :     0.0359 Val_Accuracy : 0.990800\n",
      "Step 16900 Train_Loss :     0.0026 Train_Accuracy : 1.000000 Val_Loss :     0.0324 Val_Accuracy : 0.991400\n",
      "Step 17000 Train_Loss :     0.0027 Train_Accuracy : 1.000000 Val_Loss :     0.0302 Val_Accuracy : 0.991200\n",
      "Step 17100 Train_Loss :     0.0186 Train_Accuracy : 0.980000 Val_Loss :     0.0327 Val_Accuracy : 0.991600\n",
      "Step 17200 Train_Loss :     0.0277 Train_Accuracy : 0.980000 Val_Loss :     0.0369 Val_Accuracy : 0.989400\n",
      "Step 17300 Train_Loss :     0.0162 Train_Accuracy : 1.000000 Val_Loss :     0.0321 Val_Accuracy : 0.992200\n",
      "Step 17400 Train_Loss :     0.0054 Train_Accuracy : 1.000000 Val_Loss :     0.0323 Val_Accuracy : 0.991400\n",
      "Step 17500 Train_Loss :     0.0013 Train_Accuracy : 1.000000 Val_Loss :     0.0324 Val_Accuracy : 0.990400\n",
      "Step 17600 Train_Loss :     0.0048 Train_Accuracy : 1.000000 Val_Loss :     0.0350 Val_Accuracy : 0.990800\n",
      "Step 17700 Train_Loss :     0.0006 Train_Accuracy : 1.000000 Val_Loss :     0.0347 Val_Accuracy : 0.990000\n",
      "Step 17800 Train_Loss :     0.0002 Train_Accuracy : 1.000000 Val_Loss :     0.0368 Val_Accuracy : 0.988800\n",
      "Step 17900 Train_Loss :     0.0011 Train_Accuracy : 1.000000 Val_Loss :     0.0350 Val_Accuracy : 0.990200\n",
      "Step 18000 Train_Loss :     0.1328 Train_Accuracy : 0.980000 Val_Loss :     0.0356 Val_Accuracy : 0.990800\n",
      "Step 18100 Train_Loss :     0.0026 Train_Accuracy : 1.000000 Val_Loss :     0.0348 Val_Accuracy : 0.990400\n",
      "Step 18200 Train_Loss :     0.0004 Train_Accuracy : 1.000000 Val_Loss :     0.0359 Val_Accuracy : 0.990400\n",
      "Step 18300 Train_Loss :     0.0183 Train_Accuracy : 1.000000 Val_Loss :     0.0330 Val_Accuracy : 0.991000\n",
      "Step 18400 Train_Loss :     0.0143 Train_Accuracy : 1.000000 Val_Loss :     0.0316 Val_Accuracy : 0.992600\n",
      "Step 18500 Train_Loss :     0.0089 Train_Accuracy : 1.000000 Val_Loss :     0.0332 Val_Accuracy : 0.991200\n",
      "Step 18600 Train_Loss :     0.0040 Train_Accuracy : 1.000000 Val_Loss :     0.0327 Val_Accuracy : 0.991800\n",
      "Step 18700 Train_Loss :     0.0007 Train_Accuracy : 1.000000 Val_Loss :     0.0327 Val_Accuracy : 0.991400\n",
      "Step 18800 Train_Loss :     0.0001 Train_Accuracy : 1.000000 Val_Loss :     0.0312 Val_Accuracy : 0.991200\n",
      "Step 18900 Train_Loss :     0.0880 Train_Accuracy : 0.980000 Val_Loss :     0.0315 Val_Accuracy : 0.992400\n",
      "Step 19000 Train_Loss :     0.0010 Train_Accuracy : 1.000000 Val_Loss :     0.0344 Val_Accuracy : 0.991000\n",
      "Step 19100 Train_Loss :     0.0004 Train_Accuracy : 1.000000 Val_Loss :     0.0328 Val_Accuracy : 0.990600\n",
      "Step 19200 Train_Loss :     0.0467 Train_Accuracy : 0.960000 Val_Loss :     0.0342 Val_Accuracy : 0.991200\n",
      "Step 19300 Train_Loss :     0.0037 Train_Accuracy : 1.000000 Val_Loss :     0.0325 Val_Accuracy : 0.991600\n",
      "Step 19400 Train_Loss :     0.0066 Train_Accuracy : 1.000000 Val_Loss :     0.0318 Val_Accuracy : 0.991200\n",
      "Step 19500 Train_Loss :     0.0009 Train_Accuracy : 1.000000 Val_Loss :     0.0333 Val_Accuracy : 0.990600\n",
      "Step 19600 Train_Loss :     0.0001 Train_Accuracy : 1.000000 Val_Loss :     0.0332 Val_Accuracy : 0.990000\n",
      "Step 19700 Train_Loss :     0.0017 Train_Accuracy : 1.000000 Val_Loss :     0.0324 Val_Accuracy : 0.991000\n",
      "Step 19800 Train_Loss :     0.0004 Train_Accuracy : 1.000000 Val_Loss :     0.0305 Val_Accuracy : 0.991200\n",
      "Step 19900 Train_Loss :     0.0013 Train_Accuracy : 1.000000 Val_Loss :     0.0326 Val_Accuracy : 0.992000\n",
      "\n",
      "Test accuracy : 0.9904000759124756\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "        display_stats(sess, batch[0], batch[1], mnist.validation.images, mnist.validation.labels, \n",
    "                      cross_entropy, accuracy, writer, i)\n",
    "        saver.save(sess, 'trained/test_model')\n",
    "    sess.run(train_step,feed_dict={inputs: batch[0], targets: batch[1], keep_prob: 0.5, learning_rate:0.0001})\n",
    "\n",
    "print('\\nTest accuracy : {}'.format(sess.run(accuracy,feed_dict={\n",
    "    inputs: mnist.test.images, targets: mnist.test.labels, keep_prob: 1.0, learning_rate:0.0001})))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using the Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained/test_model\n",
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd32efea9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "saver=tf.train.import_meta_graph('trained/test_model.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./trained/'))\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "output = graph.get_tensor_by_name(\"output_fc_layer/outputs:0\")\n",
    "\n",
    "inputs = graph.get_tensor_by_name(\"Inputs:0\")\n",
    "targets = graph.get_tensor_by_name(\"Labels:0\")\n",
    "keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "learning_rate = graph.get_tensor_by_name(\"learning_rate:0\")\n",
    "\n",
    "pred = tf.nn.softmax(output)\n",
    "img_predict_index = np.random.randint(mnist.test.images.shape[0])\n",
    "img_array = 255 * mnist.test.images[img_predict_index]\n",
    "img_array = img_array.astype(\"uint8\")\n",
    "plt.imshow(img_array.reshape([28,28]))\n",
    "plt.gray()\n",
    "\n",
    "predictions = sess.run(pred, feed_dict={inputs:mnist.test.images[img_predict_index].reshape(1,784), \n",
    "                                        targets: mnist.test.labels[img_predict_index].reshape(1,10), \n",
    "                                        keep_prob:1.0, learning_rate:0.0001})\n",
    "\n",
    "print(np.argmax(predictions[0]))\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
